 **Word Embedding** converts each word or token into a vector of numbers that represents its semantic meaning.
 
 ![Word_Embedding](./pictures/Word_Embedding.png)

*Figure : Word_Embedding*

Reusing the Embedding Layer: The same embedding process is applied for each word in a sentence, which allows the model to handle sentences of any length.

 ![Same_Word_Embedding](./pictures/Same_Word_Embedding.png)

*Figure : Same_Word_Embedding*

 ![Backpropagation](./pictures/Backpropagation.png)

*Figure : Backpropagation*

 ![training](./pictures/training.png)

*Figure : training*